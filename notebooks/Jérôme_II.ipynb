{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Tvr7jJDehC"
      },
      "source": [
        "# Jérôme II – Object Detection Pipeline with YOLOv8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Author:** Éloïse Delerue\n",
        "\n",
        "**Laboratory:** Université Paris Sciences & Lettres\n",
        "\n",
        "**Contact:** eloise.delerue@psl.eu\n",
        "\n",
        "**Date:** November 7th, 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTL0cwCsPO0k"
      },
      "source": [
        "## 0. Read Me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV2BFuli32N0"
      },
      "source": [
        "### 0.1. Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT4jvhuJO3wy"
      },
      "source": [
        "**Jérôme II** is a demonstration notebook implementing a full image processing and object detection pipeline using a YOLOv8 model from [Ultralytics](https://github.com/ultralytics/ultralytics). It was designed to be a follow-up to **Jérôme I** (YOLOv8 fine-tuning).\n",
        "\n",
        "This notebook loads a dataset of images, performs inference with a trained model, visualises detected objects, and optionally exports results for further analysis.\n",
        "\n",
        "**Please note that it is designed to run in Google Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEkDHqfw38a-"
      },
      "source": [
        "### 0.2. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utRBEaJh3-Nk"
      },
      "source": [
        "The dataset must be provided as a **ZIP archive containing images with the following structure**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MQU1CDzPBek"
      },
      "outputs": [],
      "source": [
        "dataset.zip\n",
        " ├── image_001.jpg\n",
        " ├── image_002.jpg\n",
        " ├── ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ars7YjhF4SoK"
      },
      "source": [
        "The dataset will be automatically extracted in the notebook environment before inference.\n",
        "\n",
        "**Please note that a zipped dataset is provided with this notebook, titled \"bacchus\", for inference; but any other properly structured dataset will do the job.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB94qE5a4TXd"
      },
      "source": [
        "### 0.3. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d9D8QsD4cuA"
      },
      "source": [
        "The notebook currently uses the YOLOv8n pre-trained model **for demonstration purposes**. You may replace yolov8n.pt with any fine-tuned YOLO model (e.g., yolov8m.pt, yolov8l.pt, or a custom checkpoint). The main objective would be to use a model resulting from the fine-tuning performed in **Jérôme I**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hLEbgHg5G9d"
      },
      "source": [
        "### 0.4. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XYEQOVDPHd9"
      },
      "source": [
        "Detection results are displayed inline using OpenCV’s cv2_imshow() function.\n",
        "Bounding boxes, labels, and confidence scores are drawn on each processed image for clarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XufDd9hq5UyV"
      },
      "source": [
        "### 0.5. Results Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuqUEwqk5iD8"
      },
      "source": [
        "The final cell allows exporting the processed images and detection results into a ZIP file.\n",
        "\n",
        "To enable export, simply **uncomment the corresponding cell in the notebook**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti-l9LM0RAXF"
      },
      "source": [
        "## 1. Setup & Libraries Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LezPqSvqLScZ",
        "outputId": "8194ab6c-c0d2-4d59-d88b-5edf97be152f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing Required Librarie\n",
        "!pip install -q ultralytics opencv-python matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nROcjr1pLbAu",
        "outputId": "9179d409-1cea-4a3f-b7ed-b7a99db566c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# Importing Libraries for File and Directory Management\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Importing Libraries for Google Colab Integration\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Importing Libraries for Computer Vision\n",
        "import cv2\n",
        "\n",
        "# Importing Ultralytics YOLO for Object Detection\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI3StvlVREAF"
      },
      "source": [
        "## 2. Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqYCxYIyRoI1"
      },
      "source": [
        "### 2.1. Visualisation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "paF8lgbGM360"
      },
      "outputs": [],
      "source": [
        "def box_label(image, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n",
        "    \"\"\"\n",
        "    Draws a bounding box and label on an image.\n",
        "\n",
        "    Args:\n",
        "        image: The image to draw on.\n",
        "        box: Bounding box coordinates as [x1, y1, x2, y2].\n",
        "        label: Label to display.\n",
        "        color: Color of the bounding box.\n",
        "        txt_color: Color of the label text.\n",
        "    \"\"\"\n",
        "    lw = max(round(sum(image.shape) / 2 * 0.003), 2)\n",
        "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "    cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n",
        "    if label:\n",
        "        tf = max(lw - 1, 1)\n",
        "        w, h = cv2.getTextSize(label, 0, fontScale=lw / 3, thickness=tf)[0]\n",
        "        outside = p1[1] - h >= 3\n",
        "        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)\n",
        "        cv2.putText(image, label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
        "                    0, lw / 3, txt_color, thickness=tf, lineType=cv2.LINE_AA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o-2YP1FOM6b1"
      },
      "outputs": [],
      "source": [
        "def plot_bboxes(image, boxes, labels=[], colors=[], score=True, conf=None):\n",
        "    \"\"\"\n",
        "    Plots bounding boxes on an image.\n",
        "\n",
        "    Args:\n",
        "        image: The image to plot on.\n",
        "        boxes: List of bounding boxes.\n",
        "        labels: Dictionary mapping class IDs to labels.\n",
        "        colors: List of colors for each class.\n",
        "        score: Whether to display confidence scores.\n",
        "        conf: Confidence threshold for filtering boxes.\n",
        "    \"\"\"\n",
        "    if not labels:\n",
        "        labels = {0: 'person'}\n",
        "    if not colors:\n",
        "        colors = [(89, 161, 197), (67, 161, 255), (19, 222, 24), (186, 55, 2), (167, 146, 11)]\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2, confidence, class_id = box\n",
        "        class_id = int(class_id)\n",
        "        if conf and confidence < conf:\n",
        "            continue\n",
        "        label = f\"{labels.get(class_id, f'class{class_id}')} {confidence*100:.1f}%\" if score else labels.get(class_id, f'class{class_id}')\n",
        "        color = colors[class_id % len(colors)]\n",
        "        box_label(image, [x1, y1, x2, y2], label, color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPP6DqOtRlqM"
      },
      "source": [
        "### 2.2. Object Detection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A_HRUbXxLDRq"
      },
      "outputs": [],
      "source": [
        "def detect_motifs(model, image_path, confidence_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Detects objects in an image using a YOLO model.\n",
        "\n",
        "    Args:\n",
        "        model: The YOLO model.\n",
        "        image_path: Path to the image.\n",
        "        confidence_threshold: Minimum confidence for detections.\n",
        "\n",
        "    Returns:\n",
        "        List of detections, each as [x1, y1, x2, y2, confidence, class_id].\n",
        "    \"\"\"\n",
        "    results = model(image_path, conf=confidence_threshold)\n",
        "    detections = []\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            confidence = float(box.conf[0])\n",
        "            class_id = int(box.cls[0])\n",
        "            detections.append([x1, y1, x2, y2, confidence, class_id])\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSI4X4s5Rs_T"
      },
      "source": [
        "## 3. Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3CMJ5HJRyFh",
        "outputId": "fd4984d4-b9d5-4bf1-906e-6e4d7dc100aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 122.1MB/s 0.1s\n"
          ]
        }
      ],
      "source": [
        "# Load the YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Or, if you have your own model file:\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Get the filename of the uploaded model\n",
        "# model_path = list(uploaded.keys())[0]\n",
        "\n",
        "# # Load your custom model\n",
        "# model = YOLO(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZQ9jmErR1i3"
      },
      "source": [
        "## 4. Images Loading & Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "inxFn3w2R8GC",
        "outputId": "46019475-3264-4d6e-cb79-e58cb81bf3cc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-875fe669-0f70-4dff-ba6b-98520a925ff3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-875fe669-0f70-4dff-ba6b-98520a925ff3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving bacchus.zip to bacchus.zip\n"
          ]
        }
      ],
      "source": [
        "# Upload and extract images\n",
        "uploaded = files.upload()\n",
        "uploaded_files = list(uploaded.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bLZC8gxjSANK"
      },
      "outputs": [],
      "source": [
        "# Extract images if a ZIP file is uploaded\n",
        "if uploaded_files and uploaded_files[0].endswith('.zip'):\n",
        "    zip_path = uploaded_files[0]\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"uploaded_images\")\n",
        "    extracted_images = [f\"uploaded_images/{f}\" for f in os.listdir(\"uploaded_images\")\n",
        "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "else:\n",
        "    extracted_images = uploaded_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldHj9rzoSK1V"
      },
      "source": [
        "## 5. Images Processing & Pattern Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CJoSQMMAR_9Q"
      },
      "outputs": [],
      "source": [
        "# Create results directory\n",
        "os.makedirs(\"results\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HDszXPrISAwk"
      },
      "outputs": [],
      "source": [
        "# List to store images with \"person\" detections and their bounding boxes\n",
        "person_images = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8twlZTM_SCWQ",
        "outputId": "be5dc16a-4f87-4f49-ccbd-88e0c2ef1293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/uploaded_images/04.jpg: 448x640 1 person, 368.3ms\n",
            "Speed: 16.1ms preprocess, 368.3ms inference, 29.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Image: uploaded_images/04.jpg\n",
            "  Bounding box: (645, 161, 763, 453), Confidence: 0.58\n",
            "\n",
            "image 1/1 /content/uploaded_images/11.jpg: 448x640 3 persons, 183.9ms\n",
            "Speed: 4.1ms preprocess, 183.9ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Image: uploaded_images/11.jpg\n",
            "  Bounding box: (573, 85, 811, 705), Confidence: 0.84\n",
            "  Bounding box: (43, 238, 275, 699), Confidence: 0.82\n",
            "  Bounding box: (442, 398, 567, 738), Confidence: 0.66\n",
            "\n",
            "image 1/1 /content/uploaded_images/53.jpg: 416x640 1 person, 2 dogs, 163.3ms\n",
            "Speed: 2.6ms preprocess, 163.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Image: uploaded_images/53.jpg\n",
            "  Bounding box: (321, 7, 633, 415), Confidence: 0.63\n",
            "\n",
            "image 1/1 /content/uploaded_images/48.jpg: 256x640 3 persons, 3 dogs, 110.1ms\n",
            "Speed: 1.8ms preprocess, 110.1ms inference, 1.9ms postprocess per image at shape (1, 3, 256, 640)\n",
            "Image: uploaded_images/48.jpg\n",
            "  Bounding box: (858, 46, 1131, 397), Confidence: 0.77\n",
            "  Bounding box: (86, 137, 321, 644), Confidence: 0.55\n",
            "  Bounding box: (1298, 57, 1653, 582), Confidence: 0.55\n",
            "\n",
            "image 1/1 /content/uploaded_images/25.jpg: 640x320 1 person, 139.0ms\n",
            "Speed: 2.1ms preprocess, 139.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Image: uploaded_images/25.jpg\n",
            "  Bounding box: (373, 181, 482, 617), Confidence: 0.65\n",
            "\n",
            "image 1/1 /content/uploaded_images/41.jpg: 640x512 2 persons, 207.1ms\n",
            "Speed: 4.1ms preprocess, 207.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Image: uploaded_images/41.jpg\n",
            "  Bounding box: (65, 307, 686, 1071), Confidence: 0.68\n",
            "  Bounding box: (311, 59, 899, 410), Confidence: 0.66\n",
            "\n",
            "image 1/1 /content/uploaded_images/55.jpg: 480x640 2 persons, 202.9ms\n",
            "Speed: 3.2ms preprocess, 202.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Image: uploaded_images/55.jpg\n",
            "  Bounding box: (1204, 287, 1643, 1063), Confidence: 0.77\n",
            "  Bounding box: (1086, 208, 1273, 586), Confidence: 0.53\n",
            "\n",
            "image 1/1 /content/uploaded_images/57.jpg: 640x512 (no detections), 189.0ms\n",
            "Speed: 3.3ms preprocess, 189.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 512)\n",
            "No person detected in uploaded_images/57.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/23.jpg: 544x640 4 persons, 1 dog, 213.5ms\n",
            "Speed: 3.4ms preprocess, 213.5ms inference, 1.8ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Image: uploaded_images/23.jpg\n",
            "  Bounding box: (790, 79, 1011, 520), Confidence: 0.72\n",
            "  Bounding box: (44, 290, 275, 791), Confidence: 0.60\n",
            "  Bounding box: (408, 339, 680, 787), Confidence: 0.51\n",
            "  Bounding box: (220, 452, 478, 805), Confidence: 0.50\n",
            "\n",
            "image 1/1 /content/uploaded_images/29.jpg: 640x544 2 persons, 211.1ms\n",
            "Speed: 3.8ms preprocess, 211.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Image: uploaded_images/29.jpg\n",
            "  Bounding box: (180, 236, 401, 499), Confidence: 0.65\n",
            "  Bounding box: (21, 75, 317, 498), Confidence: 0.63\n",
            "\n",
            "image 1/1 /content/uploaded_images/30.jpg: 512x640 4 persons, 198.0ms\n",
            "Speed: 3.4ms preprocess, 198.0ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Image: uploaded_images/30.jpg\n",
            "  Bounding box: (818, 219, 1365, 1170), Confidence: 0.66\n",
            "  Bounding box: (136, 594, 344, 1110), Confidence: 0.63\n",
            "  Bounding box: (269, 351, 773, 1170), Confidence: 0.60\n",
            "  Bounding box: (387, 117, 754, 497), Confidence: 0.56\n",
            "\n",
            "image 1/1 /content/uploaded_images/08.jpg: 640x544 1 person, 207.7ms\n",
            "Speed: 3.6ms preprocess, 207.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Image: uploaded_images/08.jpg\n",
            "  Bounding box: (111, 30, 791, 1146), Confidence: 0.66\n",
            "\n",
            "image 1/1 /content/uploaded_images/43.jpg: 640x544 1 person, 188.6ms\n",
            "Speed: 3.5ms preprocess, 188.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Image: uploaded_images/43.jpg\n",
            "  Bounding box: (237, 260, 794, 994), Confidence: 0.84\n",
            "\n",
            "image 1/1 /content/uploaded_images/37.jpg: 384x640 (no detections), 155.1ms\n",
            "Speed: 2.5ms preprocess, 155.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "No person detected in uploaded_images/37.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/54.jpg: 544x640 3 persons, 209.0ms\n",
            "Speed: 3.9ms preprocess, 209.0ms inference, 2.2ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Image: uploaded_images/54.jpg\n",
            "  Bounding box: (773, 821, 1366, 1145), Confidence: 0.74\n",
            "  Bounding box: (95, 358, 354, 699), Confidence: 0.68\n",
            "  Bounding box: (527, 535, 886, 977), Confidence: 0.54\n",
            "\n",
            "image 1/1 /content/uploaded_images/36.jpg: 544x640 (no detections), 205.3ms\n",
            "Speed: 3.6ms preprocess, 205.3ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 640)\n",
            "No person detected in uploaded_images/36.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/17.jpg: 640x576 1 person, 213.5ms\n",
            "Speed: 3.1ms preprocess, 213.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Image: uploaded_images/17.jpg\n",
            "  Bounding box: (28, 24, 425, 501), Confidence: 0.64\n",
            "\n",
            "image 1/1 /content/uploaded_images/50.jpg: 640x512 1 person, 187.4ms\n",
            "Speed: 3.2ms preprocess, 187.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Image: uploaded_images/50.jpg\n",
            "  Bounding box: (29, 83, 643, 893), Confidence: 0.76\n",
            "\n",
            "image 1/1 /content/uploaded_images/31.jpg: 640x416 1 person, 167.9ms\n",
            "Speed: 2.8ms preprocess, 167.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Image: uploaded_images/31.jpg\n",
            "  Bounding box: (393, 100, 666, 1063), Confidence: 0.52\n",
            "\n",
            "image 1/1 /content/uploaded_images/56.jpg: 448x640 3 persons, 159.4ms\n",
            "Speed: 2.9ms preprocess, 159.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Image: uploaded_images/56.jpg\n",
            "  Bounding box: (390, 117, 1142, 782), Confidence: 0.77\n",
            "  Bounding box: (2, 108, 471, 781), Confidence: 0.75\n",
            "  Bounding box: (378, 113, 753, 627), Confidence: 0.51\n",
            "\n",
            "image 1/1 /content/uploaded_images/06.jpg: 640x576 1 person, 213.4ms\n",
            "Speed: 3.6ms preprocess, 213.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Image: uploaded_images/06.jpg\n",
            "  Bounding box: (128, 146, 586, 771), Confidence: 0.66\n",
            "\n",
            "image 1/1 /content/uploaded_images/34.jpg: 512x640 (no detections), 181.0ms\n",
            "Speed: 3.5ms preprocess, 181.0ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
            "No person detected in uploaded_images/34.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/51.jpg: 352x640 1 person, 151.2ms\n",
            "Speed: 2.4ms preprocess, 151.2ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "Image: uploaded_images/51.jpg\n",
            "  Bounding box: (1063, 73, 1338, 490), Confidence: 0.72\n",
            "\n",
            "image 1/1 /content/uploaded_images/44.jpg: 640x320 (no detections), 123.2ms\n",
            "Speed: 2.2ms preprocess, 123.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 320)\n",
            "No person detected in uploaded_images/44.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/16.jpg: 640x480 3 persons, 187.6ms\n",
            "Speed: 2.9ms preprocess, 187.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Image: uploaded_images/16.jpg\n",
            "  Bounding box: (195, 177, 491, 885), Confidence: 0.86\n",
            "  Bounding box: (454, 467, 592, 884), Confidence: 0.84\n",
            "  Bounding box: (49, 313, 287, 869), Confidence: 0.59\n",
            "\n",
            "image 1/1 /content/uploaded_images/52.jpg: 544x640 2 persons, 212.2ms\n",
            "Speed: 3.6ms preprocess, 212.2ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Image: uploaded_images/52.jpg\n",
            "  Bounding box: (327, 505, 530, 924), Confidence: 0.80\n",
            "  Bounding box: (482, 602, 606, 904), Confidence: 0.66\n",
            "\n",
            "image 1/1 /content/uploaded_images/35.jpg: 576x640 2 persons, 212.1ms\n",
            "Speed: 3.8ms preprocess, 212.1ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
            "Image: uploaded_images/35.jpg\n",
            "  Bounding box: (498, 221, 1109, 1195), Confidence: 0.83\n",
            "  Bounding box: (113, 168, 734, 1141), Confidence: 0.76\n",
            "\n",
            "image 1/1 /content/uploaded_images/45.jpg: 640x448 2 persons, 1 bowl, 1 vase, 173.9ms\n",
            "Speed: 3.1ms preprocess, 173.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Image: uploaded_images/45.jpg\n",
            "  Bounding box: (252, 458, 608, 879), Confidence: 0.68\n",
            "  Bounding box: (32, 171, 604, 876), Confidence: 0.66\n",
            "\n",
            "image 1/1 /content/uploaded_images/21.jpg: 608x640 4 persons, 1 dog, 233.9ms\n",
            "Speed: 4.0ms preprocess, 233.9ms inference, 1.9ms postprocess per image at shape (1, 3, 608, 640)\n",
            "Image: uploaded_images/21.jpg\n",
            "  Bounding box: (8, 530, 255, 1060), Confidence: 0.89\n",
            "  Bounding box: (731, 456, 918, 1054), Confidence: 0.67\n",
            "  Bounding box: (572, 749, 756, 1118), Confidence: 0.61\n",
            "  Bounding box: (1104, 519, 1323, 1086), Confidence: 0.52\n",
            "\n",
            "image 1/1 /content/uploaded_images/24.JPG: 640x480 (no detections), 169.2ms\n",
            "Speed: 3.0ms preprocess, 169.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "No person detected in uploaded_images/24.JPG\n",
            "\n",
            "image 1/1 /content/uploaded_images/49.jpg: 544x640 1 person, 1 cow, 1 elephant, 201.2ms\n",
            "Speed: 4.8ms preprocess, 201.2ms inference, 1.6ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Image: uploaded_images/49.jpg\n",
            "  Bounding box: (80, 307, 195, 563), Confidence: 0.58\n",
            "\n",
            "image 1/1 /content/uploaded_images/40.jpg: 384x640 4 persons, 2 dogs, 139.4ms\n",
            "Speed: 2.6ms preprocess, 139.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Image: uploaded_images/40.jpg\n",
            "  Bounding box: (2, 297, 96, 502), Confidence: 0.83\n",
            "  Bounding box: (144, 286, 253, 527), Confidence: 0.77\n",
            "  Bounding box: (916, 373, 1015, 626), Confidence: 0.73\n",
            "  Bounding box: (86, 295, 159, 493), Confidence: 0.71\n",
            "\n",
            "image 1/1 /content/uploaded_images/28.jpg: 512x640 (no detections), 185.4ms\n",
            "Speed: 3.1ms preprocess, 185.4ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "No person detected in uploaded_images/28.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/02.jpg: 640x544 1 person, 194.0ms\n",
            "Speed: 3.9ms preprocess, 194.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Image: uploaded_images/02.jpg\n",
            "  Bounding box: (10, 67, 390, 469), Confidence: 0.84\n",
            "\n",
            "image 1/1 /content/uploaded_images/18.jpg: 640x544 (no detections), 192.5ms\n",
            "Speed: 3.7ms preprocess, 192.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "No person detected in uploaded_images/18.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/33.jpg: 640x512 1 person, 183.3ms\n",
            "Speed: 4.7ms preprocess, 183.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Image: uploaded_images/33.jpg\n",
            "  Bounding box: (177, 33, 891, 1112), Confidence: 0.66\n",
            "\n",
            "image 1/1 /content/uploaded_images/03.jpg: 320x640 4 persons, 132.4ms\n",
            "Speed: 2.1ms preprocess, 132.4ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "Image: uploaded_images/03.jpg\n",
            "  Bounding box: (890, 163, 966, 393), Confidence: 0.70\n",
            "  Bounding box: (674, 67, 866, 593), Confidence: 0.63\n",
            "  Bounding box: (0, 121, 100, 563), Confidence: 0.56\n",
            "  Bounding box: (984, 102, 1078, 405), Confidence: 0.55\n",
            "\n",
            "image 1/1 /content/uploaded_images/26.jpg: 640x576 2 persons, 203.6ms\n",
            "Speed: 3.4ms preprocess, 203.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Image: uploaded_images/26.jpg\n",
            "  Bounding box: (0, 71, 288, 739), Confidence: 0.86\n",
            "  Bounding box: (210, 255, 664, 764), Confidence: 0.68\n",
            "\n",
            "image 1/1 /content/uploaded_images/32.jpg: 640x608 1 person, 227.7ms\n",
            "Speed: 3.8ms preprocess, 227.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 608)\n",
            "Image: uploaded_images/32.jpg\n",
            "  Bounding box: (213, 122, 878, 940), Confidence: 0.65\n",
            "\n",
            "image 1/1 /content/uploaded_images/39.jpg: 640x480 3 persons, 178.3ms\n",
            "Speed: 3.0ms preprocess, 178.3ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Image: uploaded_images/39.jpg\n",
            "  Bounding box: (216, 475, 527, 916), Confidence: 0.76\n",
            "  Bounding box: (201, 612, 842, 1085), Confidence: 0.62\n",
            "  Bounding box: (30, 248, 331, 979), Confidence: 0.61\n",
            "\n",
            "image 1/1 /content/uploaded_images/07.jpg: 640x384 1 person, 153.0ms\n",
            "Speed: 2.3ms preprocess, 153.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Image: uploaded_images/07.jpg\n",
            "  Bounding box: (84, 17, 386, 704), Confidence: 0.84\n",
            "\n",
            "image 1/1 /content/uploaded_images/27.jpg: 640x640 (no detections), 244.6ms\n",
            "Speed: 4.0ms preprocess, 244.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "No person detected in uploaded_images/27.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/59.jpg: 640x512 1 person, 1 orange, 246.9ms\n",
            "Speed: 3.2ms preprocess, 246.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Image: uploaded_images/59.jpg\n",
            "  Bounding box: (20, 13, 552, 650), Confidence: 0.76\n",
            "\n",
            "image 1/1 /content/uploaded_images/01.jpg: 640x576 1 person, 310.5ms\n",
            "Speed: 5.6ms preprocess, 310.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Image: uploaded_images/01.jpg\n",
            "  Bounding box: (145, 144, 425, 569), Confidence: 0.58\n",
            "\n",
            "image 1/1 /content/uploaded_images/38.jpg: 480x640 1 person, 1 dog, 267.1ms\n",
            "Speed: 4.5ms preprocess, 267.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Image: uploaded_images/38.jpg\n",
            "  Bounding box: (461, 223, 793, 596), Confidence: 0.90\n",
            "\n",
            "image 1/1 /content/uploaded_images/12.jpg: 448x640 1 horse, 250.8ms\n",
            "Speed: 4.4ms preprocess, 250.8ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "No person detected in uploaded_images/12.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/10.jpg: 576x640 1 person, 316.5ms\n",
            "Speed: 5.7ms preprocess, 316.5ms inference, 1.8ms postprocess per image at shape (1, 3, 576, 640)\n",
            "Image: uploaded_images/10.jpg\n",
            "  Bounding box: (1, 174, 743, 1154), Confidence: 0.60\n",
            "\n",
            "image 1/1 /content/uploaded_images/22.jpg: 480x640 (no detections), 279.1ms\n",
            "Speed: 6.0ms preprocess, 279.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "No person detected in uploaded_images/22.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/20.jpg: 640x480 (no detections), 273.9ms\n",
            "Speed: 4.6ms preprocess, 273.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "No person detected in uploaded_images/20.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/13.jpg: 640x480 2 persons, 269.8ms\n",
            "Speed: 4.4ms preprocess, 269.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Image: uploaded_images/13.jpg\n",
            "  Bounding box: (4, 488, 736, 1176), Confidence: 0.88\n",
            "  Bounding box: (151, 154, 851, 1095), Confidence: 0.71\n",
            "\n",
            "image 1/1 /content/uploaded_images/42.jpg: 640x576 1 elephant, 330.7ms\n",
            "Speed: 4.9ms preprocess, 330.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "No person detected in uploaded_images/42.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/15.jpg: 416x640 2 persons, 150.7ms\n",
            "Speed: 3.3ms preprocess, 150.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Image: uploaded_images/15.jpg\n",
            "  Bounding box: (330, 130, 777, 773), Confidence: 0.65\n",
            "  Bounding box: (654, 167, 901, 480), Confidence: 0.51\n",
            "\n",
            "image 1/1 /content/uploaded_images/14.jpg: 480x640 3 persons, 174.9ms\n",
            "Speed: 2.9ms preprocess, 174.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Image: uploaded_images/14.jpg\n",
            "  Bounding box: (431, 205, 602, 695), Confidence: 0.79\n",
            "  Bounding box: (368, 121, 482, 353), Confidence: 0.62\n",
            "  Bounding box: (281, 408, 522, 703), Confidence: 0.61\n",
            "\n",
            "image 1/1 /content/uploaded_images/46.jpg: 640x544 1 person, 189.0ms\n",
            "Speed: 3.6ms preprocess, 189.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Image: uploaded_images/46.jpg\n",
            "  Bounding box: (0, 4, 644, 800), Confidence: 0.88\n",
            "\n",
            "image 1/1 /content/uploaded_images/58.jpg: 640x512 1 person, 182.2ms\n",
            "Speed: 3.5ms preprocess, 182.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Image: uploaded_images/58.jpg\n",
            "  Bounding box: (26, 108, 874, 1157), Confidence: 0.77\n",
            "\n",
            "image 1/1 /content/uploaded_images/05.jpg: 640x416 1 person, 165.7ms\n",
            "Speed: 3.0ms preprocess, 165.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Image: uploaded_images/05.jpg\n",
            "  Bounding box: (0, 97, 573, 899), Confidence: 0.77\n",
            "\n",
            "image 1/1 /content/uploaded_images/09.jpg: 448x640 (no detections), 167.9ms\n",
            "Speed: 3.1ms preprocess, 167.9ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "No person detected in uploaded_images/09.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/19.jpg: 640x544 (no detections), 199.2ms\n",
            "Speed: 3.5ms preprocess, 199.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 544)\n",
            "No person detected in uploaded_images/19.jpg\n",
            "\n",
            "image 1/1 /content/uploaded_images/47.jpg: 448x640 2 persons, 161.9ms\n",
            "Speed: 2.8ms preprocess, 161.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Image: uploaded_images/47.jpg\n",
            "  Bounding box: (559, 200, 678, 490), Confidence: 0.78\n",
            "  Bounding box: (410, 314, 512, 475), Confidence: 0.66\n"
          ]
        }
      ],
      "source": [
        "# Process each image\n",
        "for image_path in extracted_images:\n",
        "    image = cv2.imread(image_path)\n",
        "    detections = detect_motifs(model, image_path, confidence_threshold=0.5)\n",
        "    person_detections = [box for box in detections if int(box[5]) == 0]  # class_id 0 is 'person'\n",
        "\n",
        "    if person_detections:\n",
        "        plot_bboxes(image, person_detections)\n",
        "        result_path = f\"results/{os.path.basename(image_path)}\"\n",
        "        cv2.imwrite(result_path, image)\n",
        "        print(f\"Image: {image_path}\")\n",
        "        for box in person_detections:\n",
        "            x1, y1, x2, y2, confidence, _ = box\n",
        "            print(f\"  Bounding box: ({x1}, {y1}, {x2}, {y2}), Confidence: {confidence:.2f}\")\n",
        "        person_images.append({\n",
        "            \"image\": image_path,\n",
        "            \"bounding_boxes\": [{\"coordinates\": (x1, y1, x2, y2), \"confidence\": confidence} for x1, y1, x2, y2, confidence, _ in person_detections]\n",
        "        })\n",
        "    else:\n",
        "        print(f\"No person detected in {image_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlmZVaBdSEte"
      },
      "source": [
        "## 6. Processed Images Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SypBxvz-SF84",
        "outputId": "863024ab-d613-4b70-ef8f-88cf0eb1df49"
      },
      "outputs": [],
      "source": [
        "# Loop through each image path in the extracted_images list\n",
        "for image_path in extracted_images:\n",
        "    result_path = f\"results/{os.path.basename(image_path)}\"\n",
        "    if os.path.exists(result_path):\n",
        "        image = cv2.imread(result_path)\n",
        "        cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9rv2IjSI2h"
      },
      "source": [
        "## 7. Results Packaging and Downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eIkVR4NUvbr",
        "outputId": "96f4fed7-b84c-4d88-be2b-49b824180ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: results/ (stored 0%)\n",
            "  adding: results/04.jpg (deflated 4%)\n",
            "  adding: results/11.jpg (deflated 4%)\n",
            "  adding: results/53.jpg (deflated 7%)\n",
            "  adding: results/48.jpg (deflated 6%)\n",
            "  adding: results/25.jpg (deflated 4%)\n",
            "  adding: results/41.jpg (deflated 4%)\n",
            "  adding: results/55.jpg (deflated 5%)\n",
            "  adding: results/23.jpg (deflated 6%)\n",
            "  adding: results/29.jpg (deflated 4%)\n",
            "  adding: results/30.jpg (deflated 5%)\n",
            "  adding: results/08.jpg (deflated 6%)\n",
            "  adding: results/43.jpg (deflated 4%)\n",
            "  adding: results/54.jpg (deflated 4%)\n",
            "  adding: results/17.jpg (deflated 13%)\n",
            "  adding: results/50.jpg (deflated 3%)\n",
            "  adding: results/31.jpg (deflated 7%)\n",
            "  adding: results/56.jpg (deflated 7%)\n",
            "  adding: results/06.jpg (deflated 6%)\n",
            "  adding: results/51.jpg (deflated 6%)\n",
            "  adding: results/16.jpg (deflated 6%)\n",
            "  adding: results/52.jpg (deflated 0%)\n",
            "  adding: results/35.jpg (deflated 4%)\n",
            "  adding: results/45.jpg (deflated 6%)\n",
            "  adding: results/21.jpg (deflated 5%)\n",
            "  adding: results/49.jpg (deflated 5%)\n",
            "  adding: results/40.jpg (deflated 0%)\n",
            "  adding: results/02.jpg (deflated 4%)\n",
            "  adding: results/33.jpg (deflated 5%)\n",
            "  adding: results/03.jpg (deflated 4%)\n",
            "  adding: results/26.jpg (deflated 4%)\n",
            "  adding: results/32.jpg (deflated 10%)\n",
            "  adding: results/39.jpg (deflated 5%)\n",
            "  adding: results/07.jpg (deflated 8%)\n",
            "  adding: results/59.jpg (deflated 5%)\n",
            "  adding: results/01.jpg (deflated 5%)\n",
            "  adding: results/38.jpg (deflated 7%)\n",
            "  adding: results/10.jpg (deflated 7%)\n",
            "  adding: results/13.jpg (deflated 4%)\n",
            "  adding: results/15.jpg (deflated 6%)\n",
            "  adding: results/14.jpg (deflated 2%)\n",
            "  adding: results/46.jpg (deflated 7%)\n",
            "  adding: results/58.jpg (deflated 8%)\n",
            "  adding: results/05.jpg (deflated 1%)\n",
            "  adding: results/47.jpg (deflated 5%)\n"
          ]
        }
      ],
      "source": [
        "# Zip the results directory for download\n",
        "!zip -r results.zip results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ie-CgX6cSjKL",
        "outputId": "f98d9b75-bde2-47fc-c0d9-fa61d9006d9c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d83e9a1f-ed41-4daa-bc41-3e9266d89e1c\", \"results.zip\", 12566891)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download the zip file\n",
        "# files.download(\"results.zip\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
